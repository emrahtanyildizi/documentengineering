# documentengineering

post-doc research repository

Table of contents:
- Topic
- Problem Statement
- Research question
- Objective
- Key business usage
- Methods
- Topics of interest
- Keywords
- Datasets
- Papers
- Business sources
- News
- Conferences/events



TOPIC: ""Using AI to Build Document Processing Workflows""

Problem statement - Research question:

STATEMENT: Companies worldwide heavily depend on documents as a means to store and communicate vital information. Often, the transformation of this information into a digital format is essential for its utility. Nevertheless, today achieving this digitization typically involves laborious and time-consuming manual procedures that introducing inefficiencies that hinder business operations and decrease effective decision-making capabilities. The AI-based interpretation of documents such as invoices and contracts represents a profitable prospect and leading in new business opportunities. Moreover, the unstructured nature of documents introduces complexity, making it crucial to mobilize this unstructured data for a seamless and cohesive data flow. This converted data could be used for document automation, traditional analytics, BI or other downstream ML processes. Deep learning breakthroughs have driven substantial development in Large Language Models, natural language processing and computer vision, resulting in the incorporation of these approaches into modern document understanding systems. 

QUESTION: How does the implementation of LLM-driven (transformers) Document AI impact the efficiency and accuracy of transforming diverse document types into usable data, considering the variations in document structures, layouts, and information relevance? How does this transformation influence enterprises' ability to shift their operational focus from data compilation to informed decision-making?

Objective: Mobilize unstructured data/information for business needs #fitforpurpose


KEY BUSINESS USAGE:
* Completing medical intake forms during visits to medical facilities
* Processing expense reports utilizing receipts and invoices (service fee processing - fixed fees, capped fees, success-based fees, dynamic pricing (demand vs. cost)
* Verifying identity through ID card authentication
* Granting loan approvals using income details from tax forms
* Interpreting contracts relevant to significant business agreements
* Immigration document automation - RPA


Methods: 
- Large Language Model (LLM) - driven
- Generative Pre-trained Transformer (GPT)
- Key Information Extraction (KIE) / Key-Value Pair Extraction Models
- Natural Language Processing (NLP)
- Document Layout Analysis (DLA)
- Document Question Answering (DQA)
- Scientific Document Understanding (SDU)
- Optical Character Recognition (OCR)
- Table Extraction Models
- Tabular Data Comprehension (TDC)
- Robotic Process Automation (RPA)

Topics of Interest:
Document image processing / Physical and logical layout analysis / Text and symbol recognition / Handwriting recognition / Document analysis systems / Document classification / Indexing and retrieval of documents / Document synthesis / Extracting document semantics / NLP/LLM for document understanding / Human document interaction / Document Representation Modeling / Structured document generation / Multimedia document analysis / Mobile text recognition / Pen-based document analysis / Scene text detection and recognition / Recognition of tables and formulas / Historical document analysis / Signature verification / Document summarization and translation / Document forensics and provenance / Medical document analysis / Document analysis for social good / Document analysis for literature search / Gold-standard benchmarks and datasets

Keywords: Document AI, Document engineering, Document Intelligence, Intelligent document processing, document understanding, Generative AI for business documents, Deep learning, Applied data science, ADL-computer vision, Machine Learning Models, Structured/unstructured data, Tensorflow, extract text, key-value pairs, document processing, knowledge mining, automate business workflows,transformers, ...

Datasets:
Microsoft research: https://github.com/doc-analysis
XFUND:  https://github.com/doc-analysis/XFUND


Papers
2023
1- TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei AAAI 2023 | February 2023 https://arxiv.org/abs/2109.10282
Efforts to convert textual content within documents into a digital format have posed a persistent challenge in research circles. Conventional methodologies typically rely on Convolutional Neural Networks (CNNs) for comprehending images, and Recurrent Neural Networks (RNNs) for generating text on a character-by-character basis. Additionally, it's common to introduce an additional language model as a refinement step to enhance overall accuracy. However, the paper introduces a new approach, TrOCR, a comprehensive solution for text recognition that covers the entire process. The researchers harness the power of pre-trained image Transformer and text Transformer models, seamlessly integrated into a unified framework. With TrOCR, it is not solely interpreting images and constructing characters; it is also deconstructing words into smaller units for enhanced comprehension. TrOCR adopts an elegantly simple methodology, yet its impact is profound. Reseahers can initially train it on synthetically generated data and then fine-tune it using meticulously labeled human-generated datasets. Furthermore, this experiment demonstrate that TrOCR outperforms incumbent models in recognizing text across printed, handwritten, and even scene-based scenarios. The TrOCR models and code accessible at the following link https://github.com/microsoft/unilm/tree/master/trocr

2- DocILE Benchmark for Document Information Localization and Extraction, Stepan Simsa, et al. 2023 -  https://arxiv.org/abs/2302.05658    
This research paper introduces the DocILE benchmark, featuring the most extensive collection of business documents designed for two primary tasks: Key Information Localization and Extraction, as well as Line Item Recognition. The dataset encompasses 6.7k meticulously annotated business documents, supplemented by 100k synthetically generated documents, and an additional ~1M unlabeled documents to facilitate unsupervised pre-training. The construction of this dataset incorporates domain- and task-specific expertise, resulting in the following notable attributes: Annotations span across 55 distinct classes, a substantial improvement in granularity compared to previously published datasets for key information extraction. The Line Item Recognition task addresses a practical scenario where essential information must be associated with items within a table. The documents originate from a diverse range of layouts, and the test set includes instances with zero- and few-shot cases, along with layouts commonly encountered in the training set.
The benchmark is accompanied by several baseline models, including RoBERTa, LayoutLMv3, and DETR-based Table Transformer, all applied to both tasks within the DocILE benchmark. The results of these baselines are presented in the paper, serving as a valuable starting point for future research endeavors. 

2022
3- XDoc: Unified Pre-training for Cross-Format Document Understanding
Jingye Chen, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei EMNLP 2022 | December 2022 https://arxiv.org/abs/2210.02849
The recent proliferation of pre-training techniques has fueled rapid advancements in the realm of document comprehension. The pre-training and fine-tuning framework has proven highly effective in addressing texts of diverse formats, encompassing plain text, document content, and web-based textual data. Despite their promising achievements, current pre-trained models tend to specialize in specific document formats, hindering the amalgamation of insights from multiple document types. In response, the authors introduce XDoc, a holistic pre-trained model designed to seamlessly accommodate a spectrum of document formats within a single architecture. To optimize parameter utilization, they employ shared backbone parameters for distinct formats, encompassing elements like word embedding layers and Transformer components. Simultaneously, adaptive layers are introduces and characterized by lightweight parameters, to enrich the model's adaptability across varying formats. Empirical findings underscore the effectiveness of XDoc. Remarkably, with a mere 36.7% of the parameters utilized by individual pre-trained models, XDoc attains comparable, if not superior, performance across a diverse array of downstream tasks. This cost-effective efficacy positions XDoc as a valuable asset for real-world deployment. For those interested, the code and pre-trained models are accessible through the following link: https://github.com/microsoft/unilm/tree/master/xdoc

4- Business Document Information Extraction: Towards Practical Benchmarks - Matyas Skalicky, et al. 2022 - https://arxiv.org/abs/2206.11229

Efficient business-to-business (B2B) communication heavily relies on extracting information from partially structured documents. Despite the long-standing exploration of Machine Learning challenges associated with Document Information Extraction (IE), many prevailing problem formulations and performance standards fail to encompass industry-specific nuances and the tangible requirements for automating B2B document exchange. Our assessment traverses the landscape of Document IE quandaries, datasets, and performance yardsticks. We underscore the real-world aspects that elude conventional definitions and introduce the Key Information Localization and Extraction (KILE) as well as the Line Item Recognition (LIR) predicaments. The scarcity of pertinent datasets and benchmarks for Document IE concerning semi-structured business documents stems from their customary legal or sensitive content. We delve into potential resources for accessible documents, including the prospect of synthetic data.

5- DiT: Self-supervised Pre-training for Document Image Transformer
Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei ACM Multimedia 2022 | October 2022 https://arxiv.org/abs/2203.02378
The Image Transformer paradigm has achieved remarkable strides in comprehending natural images, whether through supervised methodologies (such as ViT, DeiT, etc.) or self-supervised approaches (like BEiT, MAE, etc.). This study introduces \textbf{DiT}, a self-supervised pre-trained model centered on \textbf{D}ocument \textbf{I}mage \textbf{T}ransformation. DiT capitalizes on a vast collection of unlabeled text images, a necessity for Document AI tasks where supervised counterparts are conspicuously absent, given the scarcity of human-labeled document images. DiT takes on the role of the foundational network in diverse vision-driven Document AI tasks, encompassing document image classification, document layout analysis, table detection, and text detection for OCR applications. Empirical outcomes underscore that the self-supervised, pre-trained DiT model establishes a fresh benchmark in these downstream tasks, exemplified by its superior performance in document image classification and beyond

7- LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking
Yupang Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei ACM Multimedia 2022 | October 2022 https://arxiv.org/abs/2204.08387
Significant strides have been made in Document AI through the application of self-supervised pre-training techniques. While most multimodal pre-trained models leverage masked language modeling objectives to attain bidirectional representations for the text modality, their divergence in pre-training objectives for the image modality introduces complexities to the process of multimodal representation learning. Addressing this challenge, this study presents \textbf{LayoutLMv3}, a novel approach for multimodal Transformer pre-training in the context of Document AI. Its method introduces unified text and image masking, unifying the pre-training process. Furthermore, LayoutLMv3 incorporates a word-patch alignment objective during pre-training. This alignment objective enhances cross-modal alignment by predicting whether a masked image patch corresponds to a given text word. The elegantly straightforward architecture and training objectives of LayoutLMv3 position it as a versatile pre-trained model suitable for both text-centric and image-centric Document AI tasks. Empirical findings affirm the efficacy of LayoutLMv3. It not only attains state-of-the-art performance in text-centric tasks, such as form understanding, receipt understanding, and document visual question answering, but also excels in image-centric tasks, including document image classification and document layout analysis. The code and models are available to the public via the following link: https://github.com/microsoft/unilm/tree/master/layoutlmv3

8- MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei ACL 2022 | May 2022 https://arxiv.org/abs/2110.08518
Considerable advancements have been achieved in Visually Rich Document Understanding (VRDU) through multimodal pre-training involving text, layout, and images. This progress has been particularly noticeable for documents with fixed layouts, like scanned document images. Nonetheless, a substantial volume of digital documents still exists, characterized by fluid and adaptable layouts that require interactive and dynamic rendering for effective visualization. As a result, conventional layout-focused pre-training techniques face challenges when applied to such cases. In this research paper, the authors introduce MarkupLM, a novel approach tailored for tasks related to document comprehension, specifically for documents structured using markup languages like HTML/XML. MarkupLM involves the joint pre-training of both text and markup information, forming a robust foundation. Empirical evaluations demonstrate that the pre-trained MarkupLM model consistently outperforms established benchmark models across various document understanding tasks. The model's pre-trained version and accompanying code will be made publicly accessible via the following URL https://github.com/microsoft/unilm/tree/master/markuplm.

2021
11- Document AI: Benchmarks, Models and Applications, Lei Cui and et al. 2021 - https://arxiv.org/abs/2111.08609
Emerging as a novel field of study, Document AI, or Document Intelligence, pertains to the realm of automating the interpretation, comprehension, and assessment of business documents. Positioned at the intersection of natural language processing and computer vision, it holds significant importance. The ascent of deep learning techniques in recent times has propelled the advancement of Document AI, encompassing areas like document layout analysis, visual information extraction, document visual question answering, document image classification, and more. This document provides a succinct overview of key models, tasks, and benchmark datasets that characterize this domain. Additionally, it delves into preliminary heuristic rule-based document analysis, statistical machine learning algorithms, and primarily pre-training methodologies within the purview of deep learning. Lastly, prospective avenues for future research in the field of Document AI are contemplated.

12 - Efficient Automated Processing of the Unstructured Documents Using Artificial Intelligence: A Systematic Literature Review and Future Directions, Dipali Baviskar and et al. - https://ieeexplore.ieee.org/abstract/document/9402739
Ninety-five percent of organizations are affected by unstructured data, resulting in annual costs amounting to millions of dollars. Skillful management of this data holds the potential to substantially enhance business productivity. Conventional information extraction methods have inherent limitations, whereas AI-powered approaches offer a more promising remedy. Surprisingly, a comprehensive exploration of AI-driven methods for autonomously extracting information from unstructured documents remains absent in existing literature. This Systematic Literature Review (SLR) aims to identify and scrutinize research concerning techniques employed in the automatic extraction of information from unstructured documents, while also charting pathways for future investigative efforts.


EMNLP 2022 Papers


1- Information Theory–based Compositional Distributional Semantics In Special Collection: CogNet
Enrique Amigó, Alejandro Ariza-Casabona, Victor Fresno, M. Antònia Martí December 01 2022 https://direct.mit.edu/coli/article/48/4/907/112556/Information-Theory-based-Compositional

In the realm of representing text, Compositional Distributional Semantics models have the objective of combining the Distributional Hypothesis and the Principle of Compositionality. These models create text embeddings using co-occurrence patterns, and these embeddings are then merged using compositional functions that consider the structure of the text. However, the underlying principles governing these compositional functions remain unresolved. This article introduces and investigates a concept called Information Theory–based Compositional Distributional Semantics (ICDS). The study begins by establishing formal properties for embedding, composition, and similarity functions based on Shannon’s Information Theory.
the authors evaluate existing approaches through this new perspective, determining whether they adhere to the established desirable properties. They put forth two adjustable composition and similarity functions that extend conventional methods while satisfying the formal properties. Finally, the study conducts an empirical analysis using various textual similarity datasets. These datasets encompass sentences with both high and low lexical overlap, as well as the similarity between words and their definitions. Through our theoretical examination and practical findings, we demonstrate that upholding these formal properties significantly enhances the accuracy of text representation models in terms of maintaining a consistent mapping (isometry) between the embedding and meaning spaces.

2- It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers
Zheng Tang, Mihai Surdeanu October 2022  https://arxiv.org/abs/2204.11424C
The research introduces a method to enhance explainability in relation extraction, addressing the trade-off between broad applicability and transparency. This is achieved by simultaneously training a model for relation extraction and a sequence model that annotates words within the relation's context to clarify the decisions made by the relation classifier. To offer comprehensive explanations, the model's outputs are transformed into rules, contributing to a broader understanding of the approach. The sequence model is trained using a hybrid strategy. When existing patterns provide supervision, the training is supervised. However, in cases where this supervision is lacking, a semi-supervised strategy is employed. Here, the labels of the sequence model are treated as latent variables, and the optimal assignment is learned to maximize the relation classifier's performance. The effectiveness of the proposed method is evaluated on two datasets. The results illustrate that the sequence model's labels effectively explain the decisions of the relation classifier, and notably, the joint training strategy consistently enhances the relation classifier's performance. Additionally, an evaluation of the generated rules demonstrates that these new rules complement manual rules effectively, bridging the gap between rule-based systems and neural models."

3- Enhancing Lifelong Language Learning by Improving Pseudo-Sample Generation. Kasidis Kanwatchara, Thanapapas Horsuwan, Piyawat Lertvittayakumjorn, Boonserm Kijsirikul, Peerapon Vateekul Dec. 2022 https://doi.org/10.1162/coli_a_00449
To attain lifelong language learning, pseudo-rehearsal techniques employ samples generated by a language model to refresh the knowledge pertaining to previously acquired tasks. However, if not properly managed, these methods might struggle to retain the knowledge related to intricate tasks involving lengthy texts due to the prevalence of low-quality generated samples. To address this challenge, the study puts forth three distinct contributions. Firstly, the research harnesses a pair of dual language models, each specializing in a specific aspect of the input, to create high-quality pseudo samples. Secondly, it enhances training efficiency by incorporating adapter modules, which curtail the number of parameters used. Thirdly, the study elevates the overall caliber of pseudo samples through the implementation of temporal ensembling and sample regeneration techniques. The outcomes of the study exhibits a noteworthy advancement over baseline methods across various sequences of tasks. Additionally, the analysis of pseudo samples yields valuable insights that can guide the design of even more effective pseudo-rehearsal methodologies in the future.

4- Nucleus Composition in Transition-based Dependency Parsing  Joakim Nivre, Ali Basirat, Luise Dürlich, Adam Moss   https://doi.org/10.1162/coli_a_00450
Dependency-based syntactic analysis assumes relationships between syntactic units. Computational models typically focus on word-level dependencies, but Tesnière proposed a broader nucleus concept. The study investigates enriching parsing models with this concept. The research defines nucleus within Universal Dependencies, enabling neural parsers to consider nuclei. The study across 20 languages shows that nucleus integration improves parsing accuracy, especially for main predicates, dependents, and coordination. Factors like coordination entropy and word frequency influence improvement rates. Nucleus integration fosters similarity among vectors representing the same syntactic type.

5- Effective Approaches to Neural Query Language Identification   Xingzhang Ren, Baosong Yang, Dayiheng Liu, Haibo Zhang, Xiaoyu Lv, Liang Yao, Jun Xie https://doi.org/10.1162/coli_a_00451
Query language identification (Q-LID) plays a crucial role in a cross-lingual search engine. There exist two main challenges in Q-LID: (1) insufficient contextual information in queries for disambiguation; and (2) the lack of query-style training examples for low-resource languages. The study proposes a neural Q-LID model by alleviating the above problems from both model architecture and data augmentation perspectives. Concretely, the research builds a model upon the advanced Transformer model. In order to enhance the discrimination of queries, a variety of external features (e.g., character, word, as well as script) are fed into the model and fused by a multi-scale attention mechanism. Moreover, to remedy the low resource challenge in this task, a novel machine translation–based strategy is proposed to automatically generate synthetic query-style data for low-resource languages. The research contributes the first Q-LID test set called QID-21, which consists of search queries in 21 languages. Experimental results reveal that the proposed model yields better classification accuracy than strong baselines and existing LID systems on both query and traditional LID tasks.

6- Revise and Resubmit: An Intertextual Model of Text-based Collaboration in Peer Review   Ilia Kuznetsov, Jan Buchmann, Max Eichler, Iryna Gurevych https://arxiv.org/abs/2204.10805
Peer review is a key component of the publishing process in most fields of science. The increasing submission rates put a strain on reviewing quality and efficiency, motivating the development of applications to support the reviewing and editorial work. While existing NLP studies focus on the analysis of individual texts, editorial assistance often requires modeling interactions between pairs of texts -- yet general frameworks and datasets to support this scenario are missing. Relationships between texts are the core object of the intertextuality theory -- a family of approaches in literary studies not yet operationalized in NLP. Inspired by prior theoretical work, the study proposes the first intertextual model of text-based collaboration, which encompasses three major phenomena that make up a full iteration of the review-revise-and-resubmit cycle: pragmatic tagging, linking and long-document version alignment. While peer review is used across the fields of science and publication formats, existing datasets solely focus on conference-style review in computer science. Addressing this, the research instantiates a model in the first annotated multi-domain corpus in journal-style post-publication open peer review, and provide detailed insights into the practical aspects of intertextual annotation. The resource in the study is a major step towards multi-domain, fine-grained applications of NLP in editorial support for peer review, and our intertextual framework paves the path for general-purpose modeling of text-based collaboration.

7- Hierarchical Interpretation of Neural Text Classification
Hanqi Yan, Lin Gui, Yulan He https://arxiv.org/abs/2202.09792
Recent years have witnessed increasing interests in developing interpretable models in Natural Language Processing (NLP). Most existing models aim at identifying input features such as words or phrases important for model predictions. Neural models developed in NLP however often compose word semantics in a hierarchical manner and text classification requires hierarchical modelling to aggregate local information in order to deal with topic and label shifts more effectively. As such, interpretation by words or phrases only cannot faithfully explain model decisions in text classification. This paper proposes a novel Hierarchical INTerpretable neural text classifier, called Hint, which can automatically generate explanations of model predictions in the form of label-associated topics in a hierarchical manner. Model interpretation is no longer at the word level, but built on topics as the basic semantic unit. Experimental results on both review datasets and news datasets show that the proposed approach in the study achieves text classification results on par with existing state-of-the-art text classifiers, and generates interpretations more faithful to model predictions and better understood by humans than other interpretable neural text classifiers.

8- Neural Embedding Allocation: Distributed Representations of Topic Models
Kamrun Naher Keya, Yannis Papanikolaou, James R. Foulds https://aclanthology.org/2022.cl-4.18.pdf
The study proposes a method that uses neural embeddings to improve the performance of any given LDA-style topic model. The method, called neural embedding allocation (NEA), deconstructs topic models (LDA or otherwise) into interpretable vector-space embeddings of words, topics, documents, authors, and so on, by learning neural embeddings to mimic the topic model. The research demonstrates that NEA improves coherence scores of the original topic model by smoothing out
the noisy topics when the number of topics is large. Furthermore, the study shows NEA’s effectiveness and generality in deconstructing and smoothing LDA, author-topic models, and the recent mixed membership skip-gram topic model and achieve better performance with the embeddings compared to several state-of-the-art models.

9- The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization
Ildikó Pilán, Pierre Lison, Lilja Øvrelid, Anthi Papadopoulou, David Sánchez, Montserrat Batet https://arxiv.org/abs/2202.00443
The study presents a novel benchmark and associated evaluation metrics for assessing the performance of text anonymization methods. Text anonymization, defined as the task of editing a text document to prevent the disclosure of personal information, currently suffers from a shortage of privacy-oriented annotated text resources, making it difficult to properly evaluate the level of privacy protection offered by various anonymization methods. This paper presents TAB (Text Anonymization Benchmark), a new, open-source annotated corpus developed to address this shortage. The corpus comprises 1,268 English-language court cases from the European Court of Human Rights (ECHR) enriched with comprehensive annotations about the personal information appearing in each document, including their semantic category, identifier type, confidential attributes, and co-reference relations. Compared to previous work, the TAB corpus is designed to go beyond traditional de-identification (which is limited to the detection of predefined semantic categories), and explicitly marks which text spans ought to be masked in order to conceal the identity of the person to be protected. Along with presenting the corpus and its annotation layers, the study also proposes a set of evaluation metrics that are specifically tailored towards measuring the performance of text anonymization, both in terms of privacy protection and utility preservation. The research illustrates the use of the benchmark and the proposed metrics by assessing the empirical performance of several baseline text anonymization models. The full corpus along with its privacy-oriented annotation guidelines, evaluation scripts and baseline models are available on: https://github.com/NorskRegnesentral/text-anonymization-benchmark

10- How Much Does Lookahead Matter for Disambiguation? Partial Arabic Diacritization Case Study 
Saeed Esmail, Kfir Bar, Nachum Dershowitz Computational Linguistics (2022) 48 (4): 1103–1123.
https://doi.org/10.1162/coli_a_00456
The study suggests a model for partial diacritization of deep orthographies. It focuses on Arabic, where the optional indication of selected vowels by means of diacritics can resolve ambiguity and improve readability. The partial diacritizer in the research restores short vowels only when they contribute to the ease of understandability during reading a given running text. The idea is to identify those uncertainties of absent vowels that require the reader to look ahead to disambiguate. To achieve this, two independent neural networks are used for predicting diacritics, one that takes the entire sentence as input and another that considers only the text that has been read thus far. Partial diacritization is then determined by retaining precisely those vowels on which the two networks disagree, preferring the reading based on consideration of the whole sentence over the more naïve reading-order diacritization. For evaluation, the authors prepared a new dataset of Arabic texts with both full and partial vowelization. In addition to facilitating readability, the study found that the partial diacritizer improves translation quality compared either to their total absence or to random selection. Lastly, the research investigates the benefit of knowing the text that follows the word in focus toward the restoration of short vowels during reading, and it measures the degree to which lookahead contributes to resolving ambiguities encountered while reading.

11- Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction Enrica Troiano, Laura Oberländer, Roman Klinger
The most prominent tasks in emotion analysis are to assign emotions to texts and to understand how emotions manifest in language. An observation for NLP is that emotions can be communicated implicitly by referring to events, appealing to an empathetic, intersubjective understanding of events, even without explicitly mentioning an emotion name. In psychology, the class of emotion theories known as appraisal theories aims at explaining the link between events and emotions. Appraisals can be formalized as variables that measure a cognitive evaluation by people living through an event that they consider relevant. They include the assessment if an event is novel, if the person considers themselves to be responsible, if it is in line with the own goals, and many others. Such appraisals explain which emotions are developed based on an event, e.g., that a novel situation can induce surprise or one with uncertain consequences could evoke fear. The study analyzes the suitability of appraisal theories for emotion analysis in text with the goal of understanding if appraisal concepts can reliably be reconstructed by annotators, if they can be predicted by text classifiers, and if appraisal concepts help to identify emotion categories. To achieve that, the study compiles a corpus by asking people to textually describe events that triggered particular emotions and to disclose their appraisals. Then, the research asks readers to reconstruct emotions and appraisals from the text. This setup allows to measure if emotions and appraisals can be recovered purely from text and provides a human baseline. The research's comparison of text classification methods to human annotators shows that both can reliably detect emotions and appraisals with similar performance. Therefore, appraisals constitute an alternative computational emotion analysis paradigm and further improve the categorization of emotions in text with joint models.

12- Transformers and the representation of biomedical background knowledge Oskar Wysocki, Zili Zhou, Paul O'Regan, Deborah Ferreira, Magdalena Wysocka, Dónal Landers, André Freitas
Specialised transformers-based models (such as BioBERT and BioMegatron) are adapted for the biomedical domain based on publicly available biomedical corpora. As such, they have the potential to encode large-scale biological knowledge. The study investigates the encoding and representation of biological knowledge in these models, and its potential utility to support inference in cancer precision medicine - namely, the interpretation of the clinical significance of genomic alterations. the study compares the performance of different transformer baselines; it uses probing to determine the consistency of encodings for distinct entities; and the research uses clustering methods to compare and contrast the internal properties of the embeddings for genes, variants, drugs and diseases. We show that these models do indeed encode biological knowledge, although some of this is lost in fine-tuning for specific tasks. Finally, the study analyses how the models behave with regard to biases and imbalances in the dataset.








Regenerate



Business sources: 
Google: Document AI https://cloud.google.com/document-ai
Google: DocumentAI for developers https://developers.google.com/learn/topics/document-ai
Azure Document Intelligence: https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence
Amazon: Scaling intelligent document processing workflows with AWS AI services https://aws.amazon.com/blogs/publicsector/scaling-intelligent-document-processing-workflows-aws-ai/
UiPath: https://www.uipath.com/product/document-understanding
IBM:Accelerate business decisions and processes with an AI-powered intelligent document understanding and content analysis platform https://www.ibm.com/products/watson-discovery?utm_content=SRCWW&p1=Search&p4=43700075721478226&p5=p&gclid=Cj0KCQjwldKmBhCCARIsAP-0rfx01dIg_SouZjSJc9qfIFlD3CDu3U0SOlua3VnzaF_pK8ybHxLJhR4aAgffEALw_wcB&gclsrc=aw.ds
Hypatos: Document AI: Combining NLP & machine vision for top results https://www.hypatos.ai/blog/document-ai

Petal: document analysis platform: https://www.petal.org/
Snowflake: Snowflake is making another generative AI push. Today at its annual conference, the data cloud company announced Document AI, a new large language model (LLM)-based interface that allows enterprises to quickly extract value from their barrage of documents. The move marks a major development for the data giant — which started off with a focus on structured data — and provides an easy way to mobilize useful unstructured information that often remains scattered across silos.Customers will see an end-to-end experience where they will able to have documents in Snowflake and ask structured questions from those documents — like what’s the name of the employee, what’s their address or the total value in the invoice,” Kleinerman explained in a press briefing. “This will trigger the system to take the documents, which are unstructured files, and convert them into structured data. This converted data could be used for traditional analytics, BI or other downstream ML processes
Applica: acquired by Snowflake https://www.applica.ai/
Docsumo: https://www.docsumo.com/blog/document-ai-future#:~:text=AI%2Dbased%20document%20processing%20technology,easily%20analyzed%20and%20stored%20later.


News:
Using AI To Automate Enterprise Document Processing Workflows: https://www.forbes.com/sites/forbestechcouncil/2021/04/20/using-ai-to-automate-enterprise-document-processing-workflows/?sh=1d10a02f1323


Conferences / workshops
- International Conference on Document Analysis and Recognition (ICDAR) 2023 https://icdar2023.org/program/accepted-papers/
- EMNLP 2022 https://2022.emnlp.org/
- EMNLP 2023 https://2023.emnlp.org/

